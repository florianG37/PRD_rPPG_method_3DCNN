{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of a new rPPG method\n",
    "\n",
    "## Part 2 : Notebook for the prediction of the 3D-CNN model\n",
    "\n",
    "This jupyter notebook file complements the \"Train_3DCNN_model_BPM.ipynb\" file. In this file, we can test the model predictions on real videos (one sequence at a time) and highlight logic of the future implementation into the pyVHR framework. ([Link](https://ieeexplore.ieee.org/document/9272290)) ([GitHub](https://github.com/phuselab/pyVHR))\n",
    "\n",
    "This file is based on the implementation described in the following article :\n",
    "Frédéric Bousefsaf, Alain Pruski, Choubeila Maaoui, 3D convolutional neural networks for remote pulse rate measurement and mapping from facial video, Applied Sciences, vol. 9, n° 20, 4364 (2019). ([Link](https://www.mdpi.com/2076-3417/9/20/4364)) ([GitHub](https://github.com/frederic-bousefsaf/ippg-3dcnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "\n",
    "Previously , you have to install theses python librairies :\n",
    "* tensorflow\n",
    "* matplotlib\n",
    "* scipy\n",
    "* numpy\n",
    "* opencv-python\n",
    "* Copy\n",
    "* pyVHR (0.0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#RUN ON CPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "#Tensorflow/KERAS\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.models import model_from_json\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "\n",
    "# Numpy / Matplotlib / OpenCV / Scipy / Copy\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import scipy.stats as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from copy import copy\n",
    "\n",
    "#pyVHR\n",
    "from pyVHR.signals.video import Video\n",
    "from pyVHR.datasets.dataset import Dataset\n",
    "from pyVHR.datasets.dataset import datasetFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the video & pyVHR processing\n",
    "\n",
    "\n",
    "In the pyVHR framework, we work on a processed video. The processing consists of detecting and extracting an area of interest, in order to apply our rPPGs methods on relevant data.\n",
    "\n",
    "* videoFilename : path of the video\n",
    "* return : video processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Video object\n",
    "def extractionROI(videoFilename):\n",
    "    video = Video(videoFilename)\n",
    "    video.getCroppedFaces(detector='dlib', extractor='skvideo')\n",
    "    video.setMask(typeROI='skin_adapt',skinThresh_adapt=0.20)\n",
    "    return video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model\n",
    "Load model & classes\n",
    "\n",
    "* MODEL_PATH : path of the model\n",
    "* return : \n",
    "    * model :  the model trained to make predictions\n",
    "    * freq_BPM : array containing the set of classes (representing each bpm) known by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "def loadmodel(MODEL_PATH):\n",
    "    # load data in files\n",
    "    model = model_from_json(open(f'{MODEL_PATH}/model_conv3D.json').read())\n",
    "    model.load_weights(f'{MODEL_PATH}/weights_conv3D.h5')\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # define the frequencies // output dimension (number of classes used during training)\n",
    "    freq_BPM = np.linspace(55, 240, num=model.output_shape[1]-1)\n",
    "    freq_BPM = np.append(freq_BPM, -1)     # noise class\n",
    "    return model, freq_BPM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert videoframes to a single channel array\n",
    "\n",
    "Select one channel for making prediction\n",
    "\n",
    "* video : whole video\n",
    "* model : the model trained to make predictions\n",
    "* startFrame : first frame to be read\n",
    "* return : frames normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. LOAD DATA\n",
    "def convertVideoToTable(video,model, startFrame):\n",
    "    imgs = np.zeros(shape=(model.input_shape[1], video.cropSize[0], video.cropSize[1], 1))\n",
    "\n",
    "    # channel extraction\n",
    "    if (video.cropSize[2]<3):\n",
    "        IMAGE_CHANNELS = 1\n",
    "    else:\n",
    "        IMAGE_CHANNELS = video.cropSize[2]\n",
    "\n",
    "    # load images (imgs contains the whole video)\n",
    "    for j in range(0, model.input_shape[1]):\n",
    "\n",
    "        if (IMAGE_CHANNELS==3):\n",
    "            temp = video.faces[j + startFrame]/255\n",
    "            temp = temp[:,:,1]      # only the G component is currently used\n",
    "        else:\n",
    "            temp = video.faces[j + startFrame] / 255\n",
    "\n",
    "        imgs[j] = np.expand_dims(temp, 2)\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Video\n",
    "\n",
    "Realization of a catagraphy of predictions on the video.\n",
    "This function formats the video in several sets of tests, in order to make multiple predictions. The sum of these predictions is returned.\n",
    "* video : whole video\n",
    "* model : the model trained to make predictions\n",
    "* imgs : Video sequence submitted to the prediction (including the subject's face)\n",
    "* freq_BPM : array containing the set of classes (representing each bpm) known by the model\n",
    "* stepX : horizontal step for mapping\n",
    "* stepY : vertical step for mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatingDataTest(video, model, imgs, freq_BPM, stepX, stepY):\n",
    "    \n",
    "    # output - sum of predictions\n",
    "    predictions = np.zeros(shape=(len(freq_BPM)))\n",
    "    \n",
    "    # Displacement on the x axis\n",
    "    iterationX = 0\n",
    "    # Our position at n + 1 on the X axis\n",
    "    axisX = model.input_shape[3]\n",
    "    \n",
    "    # width of video\n",
    "    width = video.cropSize[1]\n",
    "    # height of video\n",
    "    height = video.cropSize[0]\n",
    "    \n",
    "    # Browse the X axis\n",
    "    while axisX < width:\n",
    "        # Displacement on the y axis\n",
    "        axisY = model.input_shape[2]\n",
    "        # Our position at n + 1 on the Y axis\n",
    "        iterationY = 0\n",
    "        # Browse the Y axis\n",
    "        while axisY < height:\n",
    "            \n",
    "            # Start position\n",
    "            x1 = iterationX * stepX\n",
    "            y1 = iterationY * stepY\n",
    "            \n",
    "            # End position\n",
    "            x2 = x1 + model.input_shape[3]\n",
    "            y2 = y1 + model.input_shape[2]\n",
    "            \n",
    "            # Cutting \n",
    "            faceCopy = copy(imgs[0:model.input_shape[1],x1:x2,y1:y2,:])\n",
    "            \n",
    "            # randomize pixel locations\n",
    "            for j in range(model.input_shape[1]):\n",
    "                temp = copy(faceCopy[j,:,:,:])\n",
    "                np.random.shuffle(temp)\n",
    "                faceCopy[j] = temp\n",
    "            \n",
    "            # Checks the validity of cutting\n",
    "            if(np.shape(faceCopy)[1] == model.input_shape[3] and np.shape(faceCopy)[2] == model.input_shape[2]):\n",
    "                # prediction on the cut part\n",
    "                xtest = faceCopy - np.mean(faceCopy)\n",
    "                predictions = predictions + getPrediction(model,freq_BPM,xtest)\n",
    "            \n",
    "            # increments\n",
    "            axisY = y2 + model.input_shape[2]\n",
    "            iterationY = iterationY +1\n",
    "        # increments    \n",
    "        axisX = x2 + model.input_shape[3]\n",
    "        iterationX = iterationX + 1\n",
    "        \n",
    "    return predictions        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a prediction\n",
    "\n",
    "Using the model to make a prediction on a map tile\n",
    "\n",
    "* model : the model trained to make predictions\n",
    "* freq_bpm : array containing the set of classes (representing each bpm) known by the model\n",
    "* xtest : model input\n",
    "* return : A prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPrediction(model,freq_BPM, xtest):\n",
    "    idx =0\n",
    "    maxi =0\n",
    "    # model.predict\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(xtest, 0))\n",
    "    h = model(input_tensor)\n",
    "    h = h.numpy() \n",
    "    #convert prediction to binary\n",
    "    res = np.zeros(shape=(76))\n",
    "    idx = getIdx(h[0])\n",
    "    res[idx] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the index of the maximum value of a prediction\n",
    "\n",
    "* h : Array (here a prediction)\n",
    "* return : index of the maximum value of an array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIdx(h):\n",
    "    idx =0\n",
    "    maxi =-1\n",
    "    for i in range(0, len(h)):\n",
    "        if maxi < h[i]:\n",
    "            idx = i\n",
    "            maxi = h[i]\n",
    "    return idx  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the label associated with the prediction\n",
    "\n",
    "Applying the formula to transform the prediction result into a value representing the estimated heart rate (BPM)\n",
    "\n",
    "* prediction : array including the addition of all predictions\n",
    "* freq_bpm : array containing the set of classes (representing each bpm) known by the model\n",
    "* return : bpm value calculated\n",
    "\n",
    "![bpm_formula](./img/bpm_formula.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClass(prediction, freq_bpm): \n",
    "    nb_bins = 0\n",
    "    score = 0\n",
    "    for i in range(len(prediction)-1):\n",
    "        nb_bins += prediction[i]\n",
    "        score += freq_bpm[i] * prediction[i]\n",
    "        \n",
    "    bpm = score / nb_bins\n",
    "    \n",
    "    return bpm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a prediction\n",
    "\n",
    "Function to make prediction on veritable data (150 first frames only in this example)\n",
    "* videoFilename : path of the video\n",
    "* modelFilename : patch of the model\n",
    "* return :  Estimated BPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePrediction(videoFilename, modelFilename):\n",
    "    # ROI EXTRACTION\n",
    "    video = extractionROI(videoFilename)\n",
    "    # print ROI EXTRACTION\n",
    "    video.showVideo()  \n",
    "    #Load the model\n",
    "    model, freq_BPM = loadmodel(modelFilename)\n",
    "    #extract Green channel or Black & whrite channel\n",
    "    framesOneChannel = convertVideoToTable(video,model,0)\n",
    "    #Data preparation \n",
    "    Xstep = 5\n",
    "    Ystep = 5\n",
    "    prediction = formatingDataTest(video, model, framesOneChannel, freq_BPM, Xstep, Ystep)\n",
    "    print(prediction)\n",
    "    bpm = getClass(prediction, freq_BPM)\n",
    "    return bpm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation test on veritable data\n",
    "\n",
    "Test on 150 first frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25a99f8e8df4ea3b418590f802d0b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='frame', max=1533, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  0. 19.  1.  1. 19.  0.  1. 13.  1.  1. 69. 15.  3. 23.  5. 10.  2.\n",
      "  1.  1. 11.  2.  6.  1. 14.  2. 14.  6.  8.  4.  3.  1.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0. 26.]\n",
      "Prediction Video 1 : 90.8108108108108\n",
      "GT Video 1 : 92.5\n",
      "ABS DIFF Video 1 : 1.689189189189193\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a4767edf8e43309616df017a33f5a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='frame', max=1800, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0. 39.  0.  0. 32.  1.  4. 15.  1.  6. 63. 26.  6. 22.  3.  4.  0.\n",
      "  1.  1.  7.  3.  7.  0.  8.  1. 31. 15. 16.  4.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0. 39.]\n",
      "Prediction Video 2 : 89.87421383647799\n",
      "GT Video 2 : 90.0\n",
      "ABS DIFF Video 2 : 0.1257861635220081\n"
     ]
    }
   ],
   "source": [
    "# video filenames\n",
    "videoFilenames = [\"./UBFC/DATASET_2/subject1/vid.avi\", \"./UBFC/DATASET_2/subject3/vid.avi\"]\n",
    "# Ground Truth (GT) filenanmes\n",
    "GT = [\"./UBFC/DATASET_2/subject1/ground_truth.txt\",\"./UBFC/DATASET_2/subject3/ground_truth.txt\"]\n",
    "# model filename\n",
    "modelFilename = \"./final_script/model/\"\n",
    "# load model\n",
    "model, freq_BPM = loadmodel(modelFilename)\n",
    "# load dataset of videos\n",
    "dataset = datasetFactory(\"UBFC2\")\n",
    "# Window size GT\n",
    "winSizeGT = 5      \n",
    "\n",
    "# For each videos\n",
    "for i in range(0, len(videoFilenames)):\n",
    "    # prediction by model\n",
    "    prediction = makePrediction(videoFilenames[i], modelFilename)\n",
    "    print(\"Prediction Video \"+ str(i+1) +\" : \"+ str(prediction))\n",
    "    # Reality\n",
    "    sigGT = dataset.readSigfile(GT[i])\n",
    "    bpmGT, timesGT = sigGT.getBPM(winSizeGT)\n",
    "    # Format the GT\n",
    "    bpm = np.round(bpmGT)\n",
    "    bpm = bpm - 55\n",
    "    bpm = np.round(bpm / 2.5)\n",
    "    GT_value = freq_BPM[int(bpm[2])]\n",
    "    print(\"GT Video \"+ str(i+1) +\" : \"+str(GT_value))\n",
    "    # difference\n",
    "    print(\"ABS DIFF Video \"+ str(i+1) +\" : \"+str(abs(GT_value-prediction)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
